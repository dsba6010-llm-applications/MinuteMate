{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riDjpPY84fmi",
        "outputId": "5143fe14-0d88-451e-b3f1-2692eb24b06c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.45.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (2.9.1)\n",
            "Requirement already satisfied: sniffio in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: certifi in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nylKS_U36keK"
      },
      "source": [
        "To run this in Colab, you will need to set an API Key named `DSBA_LLAMA3_KEY` and `MODAL_BASE_URL`, which is the URL endpoint where the LLaMa 3 model is hosted. You will need to add `/v1/` to the `MODAL_BASE_URL` path so it will look like:\n",
        "\n",
        "```\n",
        "# MODAL_BASE_URL\n",
        "https://your-workspace-name--vllm-openai-compatible-serve.modal.run/v1/\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "If using the class API example, these will be provided to you. Otherwise you will need to get these from your Modal service.\n",
        "\n",
        "![](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wEevNCOf80GTHwptPTB4g.png)\n",
        "\n",
        "As mentioned, I have hosted `LLaMa3-8B-Instruct` model that we'll use instead of OpenAI. The reason is this avoids individual costs on the API -- only cost to me for hosting on Modal.\n",
        "\n",
        "This hosted model will **not** be up indefinitely and only for class demo purposes.\n",
        "\n",
        "If you host your own model, be sure to destroy it when you're done or you'll be charged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-dotenv in c:\\users\\haits\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.0.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#Environmental variables: Running locally\n",
        "%pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "API_KEY = os.getenv('DSBA_LLAMA3_KEY')\n",
        "MODAL_BASE_URL = os.getenv('MODAL_BASE_URL')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Environmental variables: Running in Colab\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[0;32m      5\u001b[0m api_key \u001b[38;5;241m=\u001b[39m api_key\u001b[38;5;241m=\u001b[39muserdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDSBA_LLAMA3_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m base_url \u001b[38;5;241m=\u001b[39m userdata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODAL_BASE_URL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "#Environmental variables: Running in Colab\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key = api_key=userdata.get(\"DSBA_LLAMA3_KEY\")\n",
        "base_url = userdata.get(\"MODAL_BASE_URL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LiLY95O62dOS"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = API_KEY)\n",
        "client.base_url = MODAL_BASE_URL\n",
        "\n",
        "model = \"/models/NousResearch/Meta-Llama-3-8B-Instruct\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a poetic assistant, skilled in writing satirical doggerel with creative flair.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Compose a limerick about baboons and racoons.\",\n",
        "    },\n",
        "]\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Start with the first number: 2\n",
            " 2. Add 2: 2 + 2 = 4\n",
            " 3. Add 2 again: 4 + 2 = 6\n",
            "\n",
            "The answer is 6."
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You think logically, step by step.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What is 2+ 2 +2\",\n",
        "    },\n",
        "]\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    stream=True,\n",
        ")\n",
        "\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WYclO3ZycJu"
      },
      "source": [
        "Alternatively, you can run this as a cURL command. This example shows how to run it in bash (Unix/Mac).\n",
        "\n",
        "This assumes you have set local environmental variables (e.g., `.env` with `MODAL_BASE_URL` and `DSBA_LLAMA3_KEY` and loaded them)\n",
        "\n",
        "```bash\n",
        "curl \"$MODAL_BASE_URL/chat/completions\" \\\n",
        "    -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer $DSBA_LLAMA3_KEY\" \\\n",
        "    -d '{\n",
        "        \"model\": \"/models/NousResearch/Meta-Llama-3-8B-Instruct\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
        "        ]\n",
        "    }'\n",
        "```\n",
        "\n",
        "You can also add `| jq` if you have [`jq`](https://jqlang.github.io/jq/download/) installed to have it \"pretty print\":\n",
        "```bash\n",
        "curl \"$MODAL_BASE_URL/chat/completions\" \\\n",
        "    -H \"Content-Type: application/json\" \\\n",
        "    -H \"Authorization: Bearer $DSBA_LLAMA3_KEY\" \\\n",
        "    -d '{\n",
        "        \"model\": \"/models/NousResearch/Meta-Llama-3-8B-Instruct\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n",
        "        ]\n",
        "    }' | jq\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
