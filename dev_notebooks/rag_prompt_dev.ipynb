{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install Dependencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U weaviate-client\n",
    "%pip install python-dotenv\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Environmental Variables Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIRST** Create your .env file in this folder.  Use the .env.example file as a guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Environmental Variables\n",
    "\n",
    "#Environmental variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#Keys and URLs for Embedding and Generative Models\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# OPENAI_BASE_URL = os.getenv('OPENAI_BASE_URL')\n",
    "OPENAI_EMBEDDING_URL = os.getenv('OPENAI_EMBEDDING_URL')\n",
    "OPENAI_GENERATION_URL = os.getenv('OPENAI_GENERATION_URL')\n",
    "\n",
    "#Keys and URLS for Vector Databases\n",
    "WEAVIATE_URL = os.getenv('WEAVIATE_URL_VERBA')\n",
    "WEAVIATE_API_KEY = os.getenv('WEAVIATE_API_KEY_VERBA')\n",
    "\n",
    "\n",
    "print(len(WEAVIATE_URL)>20)\n",
    "print(len(WEAVIATE_API_KEY)>20)\n",
    "print(len(OPENAI_API_KEY)>20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vector Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = \"VERBA_Embedding_text_embedding_3_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTOR DATABASE CONNECTION\n",
    "\n",
    "from weaviate.classes.init import Auth, AdditionalConfig, Timeout\n",
    "import weaviate\n",
    "\n",
    "client_db = weaviate.connect_to_weaviate_cloud(\n",
    "    cluster_url = WEAVIATE_URL,\n",
    "    auth_credentials = Auth.api_key(WEAVIATE_API_KEY),\n",
    "    additional_config=AdditionalConfig(timeout=Timeout(init=30, query=60, insert=120))  # Values in seconds\n",
    ")\n",
    "\n",
    "print(client_db.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for collection in client_db.collections.list_all():\n",
    "    print(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding Service Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/docs/api-reference/embeddings/create\n",
    "\n",
    "# Extracts the actual vector embedding from the OpenAI response\n",
    "def openai_extract_vector(\n",
    "        response\n",
    "    ) -> list[float]:\n",
    "\n",
    "    return response.data[0].embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "EMBEDDING_FORMAT = \"float\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDING CONNECTION\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set API Key.  Not necessary if you have an \n",
    "# OPENAI_API_KEY variable in your environment\n",
    "openai.api_key = OPENAI_API_KEY \n",
    "\n",
    "client_embedding = OpenAI()\n",
    "\n",
    "print(type(client_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in client_embedding.models.list():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generation Service Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATION CONNECTION\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set API Key.  Not necessary if you have an \n",
    "# OPENAI_API_KEY variable in your environment\n",
    "openai.api_key = OPENAI_API_KEY \n",
    "\n",
    "generation_client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "query_text = \"I'd like to know about issues with plumbing in or around 2024\"\n",
    "\n",
    "# Vectorize the query\n",
    "response_embedding = client_embedding.embeddings.create(\n",
    "    model = EMBEDDING_MODEL,\n",
    "    input = query_text,\n",
    "    encoding_format =\"float\"\n",
    ")\n",
    "\n",
    "# Extract the verctor embeddings list[float] from the embedding response\n",
    "query_vector = openai_extract_vector(response_embedding) \n",
    "\n",
    "# Look up the appropriate Weviate database collection - name based on embedding model used\n",
    "collection = client_db.collections.get('VERBA_Embedding_text_embedding_3_small')\n",
    "\n",
    "# Do vector query\n",
    "response_db = collection.query.near_vector(\n",
    "    near_vector=query_vector,\n",
    "    limit=10,\n",
    "    return_metadata=MetadataQuery(distance=True)\n",
    ")\n",
    "\n",
    "# # Print results\n",
    "# for item in response_db.objects:\n",
    "#     print(item.properties)\n",
    "#     print(item.metadata.distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_content = \"\"\n",
    "\n",
    "for item in response_db.objects:\n",
    "    segment = '\\n<ContextSegment' + str(int(item.properties.get('chunk_id'))) + '>\\n'\n",
    "    response_content += segment\n",
    "    response_content += item.properties.get('content')\n",
    "\n",
    "# print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generation_response = generation_client.chat.completions.create(\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"You are a helpful assistant. You use the following context to generate a response: {response_content}\"},\n",
    "    {\"role\": \"user\", \"content\": query_text }\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(generation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
